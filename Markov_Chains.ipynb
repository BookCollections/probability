{
 "metadata": {
  "name": "Markov_Chains"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Markov Chains</h1>\n",
      "The following sections describe *Markov Chains* which are a type of stochastic processes where the past of the system affects the future in a way that is contained in the current system *state*.\n",
      "\n",
      "<h2>Discrete Markov Chains</h2>\n",
      "First consider **discrete-time Markov chains** in which the system state changes at discrete time instants indexed by an integer value *n*. At each time step, *n*, the state is denoted by $X_n\\hspace{1pt}$, where $X_n\\hspace{1pt}$ belongs to a finite set *S* of possible states called the **state space**. \n",
      "\n",
      "The Markov chain is described by a set of **transition probabilities** $p_{ij}\\hspace{1pt}$ that indicate the probability of the system moving from the current state, *i*, to some state *j*, with $j=i\\hspace{1pt}$ allowed, that is\n",
      "\n",
      "$$p_{ij} = P\\left(X_{n+1} = j \\vert X_n = i\\right)$$\n",
      "\n",
      "where the transition probabilities also satisfy the **Markov property**\n",
      "\n",
      "$$P\\left( X_{n+1} = j \\vert X_n=i, X_{n-1} = i_{n-1}, \\ldots, X_0=i_0\\right) = P\\left(X_{n+1} = j \\vert X_n = i\\right) = p_{ij}$$\n",
      "\n",
      "that is the additional information on the state of the system in the past does affect the transition probability. This is equivalent to saying that the transition probability is *path independent*.\n",
      "\n",
      "The elements of a Markov chain model can be summarized in a **transition probability matrix**\n",
      "\n",
      "$$\\left[ \\begin{eqnarray}p_{11} \\quad p_{12} \\quad \\ldots \\quad p_{1m} \\cr\n",
      "                        p_{21} \\quad p_{22} \\quad \\ldots \\quad p_{2m} \\cr\n",
      "                       \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\cr\n",
      "                        p_{m1} \\quad p_{m2} \\quad \\ldots \\quad p_{mm}\n",
      "\\end{eqnarray}\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>n-Step Transition Probabilities</h3>\n",
      "The probability that a discrete Markov chain will be in some specified state at *n* time steps into the future is described by the *n-* **step transition probabilities**\n",
      "\n",
      "$$r_{ij}\\left(n\\right) = P\\left(X_n =j \\vert X_0 = i \\right)$$\n",
      "\n",
      "that is the probability that the system will be in state *j* after *n* time steps given that the current state is *i*. \n",
      "\n",
      "**Chapman-Kolmogorov Equation for the n-Step Transition Probabilities**\n",
      "\n",
      "The n-step transition probabilities can be generated by the recursive formula\n",
      "\n",
      "$$r_{ij}\\left(n\\right) = \\sum_{k=1}^m r_{ik}\\left(n-1\\right)p_{kj} \\quad \\mbox{for} \\quad n>1, \\quad \\mbox{and all} \\hspace{2pt} i,j$$\n",
      "\n",
      "starting with $r_{ij}\\left(1\\right) = p_{ij}$.\n",
      "\n",
      "The n-step transition probability elements can also be summarized in matrix where $r_{ij}\\left(n\\right)\\hspace{1pt}$ is the element at the *i* th row and *j* th column. This matrix is the *n* th power of the transition probability matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Classification of States</h2>\n",
      "A state *j* is said to be **accessible** from a state *i* if for some *n*, the n-step transition probability $r_{ij}\\left(n\\right)\\hspace{1pt}$ is positive\n",
      "\n",
      "Let $A\\left(i\\right)\\hspace{1pt}$ be the set of states that are accessible from *i*. We say *i* is **recurrent** if for **every** *j* that is accessible from *i*, *i* is also accessible from *j*. A state is called **transient** if it is not recurrent, that is if there is a state $j\\in A\\left(i\\right)\\hspace{1pt}$ such that *i* is **not** accessible from *j*. \n",
      "\n",
      "If *i* is a recurrent state, the set of states $A\\left(i\\right)\\hspace{1pt}$ that are accessible from *i* form a **recurrent class**, meaning that the states in $A\\left(i\\right)\\hspace{1pt}$ are all accessible from each other. \n",
      "\n",
      "**Periodicity**\n",
      "\n",
      "A recurrent class is said to be **periodic** if its states can be grouped in $d\\gt 1\\hspace{1pt}$ disjoint **subsets** $S_1,\\ldots,S_d\\hspace{1pt}$ so that **all transitions from one subset lead to the next subset in order**. That is in a periodic recurrent class, the system moves through the sequence of **subsets in order** and after *d* steps, ends up in the original subset (not necessarily the original state, but rather some state in the original subset).\n",
      "\n",
      "The class is **aperiodic** iff there exists a time *n* such that $r_{ij}\\left(n\\right)\\gt 0\\hspace{1pt}$ for $i,j\\in R$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Steady-State Behavior</h2>\n",
      "Consider a Markov chain with a **single recurrent class**, which is **aperiodic**. Then, the states *j* are associated with steady-state probabilities, $\\pi_j\\hspace{1pt}$ having the following properties:\n",
      "\n",
      "**(a)** For each *j*, \n",
      "\n",
      "$$ \\lim_{n \\rightarrow \\infty} r_{ij}\\left(n\\right) = \\pi_j \\quad \\forall \\hspace{2pt}i $$\n",
      "\n",
      "**(b)** The $\\pi_j\\hspace{1pt}$ are the unique solution to the sytem of equations\n",
      "$$\\pi_j = \\sum_{k=1}^m \\pi_k p_{kj} \\quad j=1, \\ldots, m$$\n",
      "$$1 = \\sum_{k=1}^m \\pi_k$$\n",
      "\n",
      "where the *m* equations in the first set are *linearly dependent* and hence one can be discarded in solving the system for the $\\pi_j\\hspace{1pt}$ values. It does not matter which is discarded. \n",
      "\n",
      "**(c)** We also have\n",
      "\n",
      "$\\pi_j = 0\\hspace{1pt}$ for all transient states *j*\n",
      "\n",
      "$\\pi_j \\gt 0\\hspace{1pt}$ for all recurrent states *j*\n",
      "\n",
      "The steady-state probabilities form a probability distribution on the state space called the **stationary distribution**. That is, if the intial state is chosen according to the stationary distribution, i.e. $P\\left(X_0 = j\\right) = \\pi_j\\hspace{1pt}$ then the state at **any** future time will have the same distribution. \n",
      "\n",
      "There is a subtlety here. Consider a Markov system where the state is observed at some time *n*. The system is then restarted at some randomly selected state and again observed at time *n*. The probability that the system is in state *j* for a random observation at time *n* is given by the system state distribution at time *n*. Assuming the Markov chain is aperiodic with a single recurrent classes, then the system state distribution goes to the steady state distribution $\\pi_j \\hspace{2pt} \\forall \\hspace{2pt}j\\hspace{1pt}$ as *n* goes to infinitity *regardless of how the initial state is chosen*. But this is not necessarily true for all finite *n*. If, however, the initial state is chosen according to the steady state distribution, then the system state distribution at time *n* is the steady state distribution for *any value of n*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Steady-State Probabilities as Expected State Frequencies</h3>\n",
      "For a Markov chain with a single aperiodic class, the steady state probabilites, $\\pi_j\\hspace{1pt}$ satisfy \n",
      "\n",
      "$$\\pi_j = \\lim_{n\\rightarrow \\infty} \\frac{v_{ij}\\left(n\\right)}{n}$$\n",
      "\n",
      "where $v_{ij}\\left(n\\right)\\hspace{1pt}$ is the expected value of the number of visits to state *j* within the first *n* transitions.\n",
      "\n",
      "Based on this interpretation, $\\pi_j \\hspace{1pt}$ is the long-term expected fraction of time that the system is in state *j*. \n",
      "\n",
      "<h3>Expected Frequency of a Particular Transition</h3>\n",
      "Consider *n* transitions of a Markov chain with a single class which is aperiodic, starting from a given initial state. Let $q_{jk}\\left(j\\right)\\hspace{1pt}$ be the expected number of transitions from *j* to *k*. Then regardless of the initial state\n",
      "\n",
      "$$\\lim_{n\\rightarrow \\infty} \\frac{q_{ij}\\left(n\\right)}{n} = \\pi_j p_{jk}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Birth-Death Process</h3>\n",
      "A birth-death process is a Markov chain in which the states are linearly arranged and transitions can only occur to a neighboring state or else leave the state unchanged. In any trajectory of the Markov chain, a transition from *i* to *i+1* has to followed by a transition from *i+1* to *i* before another transition from *i* to *i+1* can occur. Thus, the expected frequency of transitions from *i* to *i+1* must equal the expected frequency of transitions from *i+1* to *i*. Letting $b_i\\hspace{1pt}$ be the probability of transition from *i* to *i+1* and $d_{i+1}\\hspace{1pt}$ be the probability of transition from *i+1* to *i*, then there is a local balance equation\n",
      "\n",
      "$$\\pi_i b_i = \\pi_{i+1}d_{i+1}$$ \n",
      "\n",
      "from which we obtain\n",
      "\n",
      "$$\\pi = \\pi_0 \\frac{b_0 b_1 \\cdots b_{i-1}}{d_1 d_2 \\cdots d_i}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}